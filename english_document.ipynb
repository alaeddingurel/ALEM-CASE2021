{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "english-document",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzZi6vkQxwI2"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers datasets seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FwE2rrgQpL1"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "df = pd.read_json(\"es-train.json\", lines=True)\n",
        "#df_test = pd.read_json(\"test.json\", lines=True)\n",
        "\n",
        "del df[\"id\"]\n",
        "X = df.text\n",
        "y = df.label\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36VruHP61ToT"
      },
      "source": [
        "import os\n",
        "%env MY_SEED=42\n",
        "seed = int(os.environ['MY_SEED'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z77eb_hzOssp"
      },
      "source": [
        "#from imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\n",
        "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# X_resampled, y_resampled = ADASYN(random_state=42).fit_resample(X_tfid, y)\n",
        "\n",
        "train, test = train_test_split(df , test_size=0.2, random_state=seed)\n",
        "#ros = RandomOverSampler(random_state=seed)\n",
        "\n",
        "#X_train, y_train = ros.fit_resample(train.text.to_numpy().reshape(-1, 1), train.label)\n",
        "\n",
        "#train = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "#train.text = X_train.flatten()\n",
        "#train.label = y_train\n",
        "\n",
        "\"\"\"\n",
        "train = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "train.text = df.text\n",
        "train.label = df.label\n",
        "test = pd.DataFrame(columns=[\"text\", \"label\"])\n",
        "test.text = df_test.text\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHJDoMET6ToR"
      },
      "source": [
        "# import nltk\n",
        "# nltk.download('punkt')\n",
        "\n",
        "\n",
        "# def sent_token(txt):\n",
        "#   l = nltk.sent_tokenize(txt)\n",
        "#   return [\" \".join(l[:len(l)//2]), \" \".join(l[len(l)//2:])]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YIcKoMjSDB2"
      },
      "source": [
        "# train.to_json(\"train-en-updated.json\", orient=\"records\", lines=True)\n",
        "# test.to_json(\"test-en-updated.json\", orient=\"records\", lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db7VlbpGToyV"
      },
      "source": [
        "!rm run_glue.py*\n",
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/text-classification/run_glue.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyWCoHSIsn8n"
      },
      "source": [
        "# cache_iterator_len = filter(lambda x: x[1] > 600, cache)\n",
        "# cache_iterator_val = filter(lambda x: x[0].label == 1, cache)\n",
        "# lcache_fil_len = list(cache_iterator_len)\n",
        "# lcache_fil_v = list(cache_iterator_val)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUrq4uDDvAn8"
      },
      "source": [
        "# from tqdm import tqdm\n",
        "# import textwrap\n",
        "# from transformers import BertTokenizer\n",
        "\n",
        "# df = pd.read_json(\"en-train.json\", lines=True)\n",
        "\n",
        "# del df[\"id\"]\n",
        "# X = df.text\n",
        "# y = df.label\n",
        "\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# dfc = df.copy()\n",
        "\n",
        "# cache = []\n",
        "# for i, v in tqdm(dfc.iterrows(), total=dfc.shape[0]):\n",
        "#   input = tokenizer.encode(v.text)\n",
        "#   tok = tokenizer.tokenize(tokenizer.decode(input))\n",
        "#   if len(tok) > 512:\n",
        "#     cache.append((v, len(tok)))\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjdLmVKPxcPK"
      },
      "source": [
        "#def modify(txt):\n",
        "#  return textwrap.wrap(txt, len(txt)/3)[1]\n",
        "#\n",
        "#for d, i in cache: \n",
        "#  new_text = modify(v.text)\n",
        "#\n",
        "#  if False:\n",
        "#    input = tokenizer.encode(new_text)\n",
        "#    tok = tokenizer.tokenize(tokenizer.decode(input))\n",
        "#    if len(tok) > 256:\n",
        "#      pass\n",
        "# \n",
        "#  dfc.loc[d._name, \"text\"] = new_text\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2hh3O9G9pvg"
      },
      "source": [
        "#def modify(txt):\n",
        "#  return textwrap.wrap(txt, len(txt)/3)[1]\n",
        "\n",
        "\n",
        "# dfc = df.copy()\n",
        "\n",
        "# for d, i in cache: \n",
        "#   new_text_list = sent_token(d.text)\n",
        "\n",
        "\n",
        "#   if False:\n",
        "#     input = tokenizer.encode(new_text)\n",
        "#     tok = tokenizer.tokenize(tokenizer.decode(input))\n",
        "#     if len(tok) > 256:\n",
        "#       pass\n",
        " \n",
        "#   dfc.drop(d._name, inplace=True)\n",
        "#   for txt in new_text_list:\n",
        "#     dfc = dfc.append({\"text\": txt, \"label\": d.label}, ignore_index=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ji7IfZ-Z47HH"
      },
      "source": [
        "!rm train-en-updated.json test-en-updated.json\n",
        "train.to_json(\"train-en-updated.json\", orient=\"records\", lines=True)\n",
        "test.to_json(\"test-en-updated.json\", orient=\"records\", lines=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g941nQgM0L5v"
      },
      "source": [
        "!rm -rf ./test_file/ ./runs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf6Iah19Diq0"
      },
      "source": [
        "%env MY_MODEL=bert-base-cased\n",
        "%env MY_EPOCH=5\n",
        "%env MY_BATCH=32\n",
        "%env MY_METHOD=RandomOverSampling-auto"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XvmBsYqHz8XN"
      },
      "source": [
        "!env | grep MY"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2nNlcc8szZc_"
      },
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path $MY_MODEL \\\n",
        "  --train_file ./train-en-updated.json \\\n",
        "  --validation_file ./train-en-updated.json \\\n",
        "  --output_dir ./test_file \\\n",
        "  --num_train_epochs $MY_EPOCH \\\n",
        "  --do_train \\\n",
        "  --per_device_train_batch_size $MY_BATCH"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJh8T7KTzFYM"
      },
      "source": [
        "!python run_glue.py \\\n",
        "  --model_name_or_path ./test_file/ \\\n",
        "  --train_file ./train-en-updated.json \\\n",
        "  --validation_file ./test-en-updated.json \\\n",
        "  --test_file ./test-en-updated.json \\\n",
        "  --output_dir ./test_file \\\n",
        "  --per_device_train_batch_size $MY_BATCH \\\n",
        "  --do_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBRM84sbBzZ5"
      },
      "source": [
        "#from transformers import BertForSequenceClassification\n",
        "\n",
        "\n",
        "#model = BertForSequenceClassification.from_pretrained(\"./test_file/\")\n",
        "#model.,\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification, AutoModelForSequenceClassification\n",
        "from transformers import PretrainedConfig\n",
        "\n",
        "from transformers import TokenClassificationPipeline, TextClassificationPipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"./test_file/\")\n",
        "\n",
        "config = PretrainedConfig.from_pretrained(\"./test_file/\")\n",
        "\n",
        "pipeline = pipeline('sentiment-analysis', device=0, model=model, tokenizer=tokenizer, config=config)\n",
        "\n",
        "nlp_grouped = TextClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMr-mYa2Elqy"
      },
      "source": [
        "\"\"\"from collections import Counter\n",
        "preds = []\n",
        "\n",
        "\n",
        "k = 0\n",
        "for i in list(test.text):\n",
        "  k += 1\n",
        "  if k % 24 == 0:\n",
        "    print(k)\n",
        "  res = pipeline(i, truncation=True, max_length=512)\n",
        "  preds.append(res[0])\n",
        "\n",
        "\n",
        "preds\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBOttwNPFnoG"
      },
      "source": [
        "\"\"\"import json\n",
        "df_test = pd.read_json(\"test.json\", lines=True)\n",
        "\n",
        "with open(\"submission.json\", \"w\") as f:\n",
        "  for i, row in df_test.iterrows():\n",
        "    if preds[i][\"label\"] == \"LABEL_1\":\n",
        "      pred = 1\n",
        "    else:\n",
        "      pred = 0\n",
        "    j = {\"prediction\": pred, \"id\":row[\"id\"]}\n",
        "    f.write(json.dumps(j) + \"\\n\")\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0KnBlaLGyfkh"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "pred = pd.read_csv(\"./test_file/predict_results_None.txt\", sep=\"\\t\")\n",
        "pred"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_U7lEblhybd_"
      },
      "source": [
        "from sklearn.metrics import f1_score, recall_score, precision_score\n",
        "\n",
        "f1macro_out = f1_score(list(test.label), list(pred.prediction), average=\"macro\")\n",
        "recall_out = recall_score(list(test.label), list(pred.prediction), average=None)\n",
        "recall_out_avg = recall_score(list(test.label), list(pred.prediction))\n",
        "\n",
        "\n",
        "recall_out_zeros = recall_out[0]\n",
        "recall_out_ones = recall_out[1]\n",
        "\n",
        "precision_out = precision_score(list(test.label), list(pred.prediction), average=None)\n",
        "precision_out_avg = precision_score(list(test.label), list(pred.prediction))\n",
        "\n",
        "precision_out_zeros = precision_out[0]\n",
        "precision_out_ones = precision_out[1]\n",
        "\n",
        "print(\"f1-macro\", f1macro_out)\n",
        "print(\"recall_avg\", recall_out_avg)\n",
        "print(\"recall_zeros\", recall_out_zeros, \"recall_ones\", recall_out_ones)\n",
        "print(\"precision_avg\", precision_out_avg)\n",
        "print(\"precision_zeros\", precision_out_zeros, \"precision_ones\", precision_out_ones)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIU7wHtsxQ2f"
      },
      "source": [
        "OUTNAME = \"results.json \"\n",
        "\n",
        "# out = pd.DataFrame(columns=[\"model\", \"epoch\", \"batch\", \"f1-macro\", \"recall\", \"precision\", \"method\", \"seed\", \"date\"])\n",
        "import time\n",
        "import json\n",
        "# out = out.append({\n",
        "#     \"model\"  : os.environ['MY_MODEL'],\n",
        "#     \"epoch\"  : os.environ['MY_EPOCH'],\n",
        "#     \"batch\"  : int(os.environ['MY_BATCH']),\n",
        "#     \"f1-macro\": f1macro_out,\n",
        "#     \"recall\": recall_out,\n",
        "#     \"precision\": precision_out,\n",
        "#     \"seed\"  : int(os.environ['MY_SEED']),\n",
        "#     \"method\" : os.environ['MY_METHOD'],\n",
        "#     \"date\": time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "#     }, ignore_index=True)\n",
        "\n",
        "out =  {\n",
        "    \"model\"  : os.environ['MY_MODEL'],\n",
        "    \"epoch\"  : os.environ['MY_EPOCH'],\n",
        "    \"batch\"  : int(os.environ['MY_BATCH']),\n",
        "    \"f1-macro\": f1macro_out,\n",
        "    \"recall\": list(recall_out),\n",
        "    \"precision\": list(precision_out),\n",
        "    \"seed\"  : int(os.environ['MY_SEED']),\n",
        "    \"method\" : os.environ['MY_METHOD'],\n",
        "    \"date\": time.strftime(\"%Y-%m-%d %H:%M\"),\n",
        "    \"prediction\": list(pred.prediction),\n",
        "    }\n",
        "\n",
        "\n",
        "with open(OUTNAME, \"a+\") as f:\n",
        "  f.write(json.dumps(out))\n",
        "  f.write(\"\\n\")\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ooN0cDzQKCB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}