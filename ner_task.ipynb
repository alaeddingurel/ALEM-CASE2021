{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ner_task",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME9r4UBoxINL"
      },
      "source": [
        "!tar -zxvf tr.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3VvdtkvxZh2"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/huggingface/transformers/master/examples/pytorch/token-classification/run_ner.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzZi6vkQxwI2"
      },
      "source": [
        "!pip install git+https://github.com/huggingface/transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LA6q2riyRqu"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Poi99NsyytRK"
      },
      "source": [
        "!mv train.csv train\n",
        "!mv test.csv test\n",
        "!mv dev.csv dev\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-iKIrOu2iEJ"
      },
      "source": [
        "import csv\n",
        "import json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPSnSmCi1PD4"
      },
      "source": [
        "def convert_to_csv(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "   data = f.readlines()\n",
        "  \n",
        "  #data = filter(lambda x : x != \"SAMPLE_START\\tO\\n\", data)\n",
        "  #data = list(data)\n",
        "  print(data[0:24])\n",
        "  for item in data[0:40]:\n",
        "   print(item.split())\n",
        "  \n",
        "  \n",
        "  with open(filename + '.json', 'w') as csvfile:\n",
        "    words = []\n",
        "    ner = []\n",
        "    for item in data:\n",
        "\n",
        "      item = item.split()\n",
        "      if not item or item[0] == \"[SEP]\":\n",
        "          d = {}\n",
        "          d[\"words\"] = words\n",
        "          d[\"ner\"] = ner\n",
        "          csvfile.write(json.dumps(d) + \"\\n\")\n",
        "          words = []\n",
        "          ner = []\n",
        "      else:\n",
        "        words.append(item[0])\n",
        "        ner.append(item[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Bu_Pc6N6vM5"
      },
      "source": [
        "def convert_to_csv_updated(filename):\n",
        "  with open(filename, \"r\") as f:\n",
        "   data = f.readlines()\n",
        "  \n",
        "  #data = filter(lambda x : x != \"SAMPLE_START\\tO\\n\", data)\n",
        "  #data = list(data)\n",
        "  print(data[0:24])\n",
        "  for item in data[0:40]:\n",
        "   print(item.split())\n",
        "  \n",
        "  \n",
        "  with open(filename + '.json', 'w') as csvfile:\n",
        "    words = []\n",
        "    ner = []\n",
        "    for item in data:\n",
        "\n",
        "      item = item.split()\n",
        "      if not item or item[0] == \"[SEP]\":\n",
        "          d = {}\n",
        "          d[\"words\"] = words\n",
        "          d[\"ner\"] = []\n",
        "          csvfile.write(json.dumps(d) + \"\\n\")\n",
        "          words = []\n",
        "          ner = []\n",
        "      else:\n",
        "        words.append(item[0])\n",
        "        #ner.append(item[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL5IRj772-Sy"
      },
      "source": [
        "\n",
        "convert_to_csv(\"pr-train.txt\")\n",
        "#convert_to_csv(\"test\")\n",
        "#convert_to_csv(\"dev\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tllC-4XE6A6F"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_json(\"pr-train.txt.json\", lines=True)\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train.to_json(\"train.json\", lines=True, orient='records')\n",
        "test.to_json(\"test.json\", lines=True, orient='records')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASuF5E9D4Vte"
      },
      "source": [
        "!cat train.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tpu2BuA8CMSk"
      },
      "source": [
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ro4rxYFxz5O"
      },
      "source": [
        "!python run_ner.py \\\n",
        "  --model_name_or_path neuralmind/bert-base-portuguese-cased \\\n",
        "  --train_file ./train.json \\\n",
        "  --validation_file ./test.json \\\n",
        "  --return_entity_level_metrics \\\n",
        "  --output_dir ./test-ner \\\n",
        "  --do_train \\\n",
        "  --do_eval\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyGNrIE26iG0"
      },
      "source": [
        "convert_to_csv_updated(\"test.txt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZcBKfKtYB_zk"
      },
      "source": [
        "!python run_ner.py \\\n",
        "  --model_name_or_path ./test-ner \\\n",
        "  --train_file ./train.json \\\n",
        "  --validation_file ./train.json \\\n",
        "  --test_file ./test.txt.json \\\n",
        "  --output_dir ./test_file \\\n",
        "  --tokenizer_name bert-base-uncased \\\n",
        "  --per_device_train_batch_size 32 \\\n",
        "  --do_predict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EjKe1l0DPIU"
      },
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import PretrainedConfig\n",
        "\n",
        "from transformers import TokenClassificationPipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"./test-ner\")\n",
        "\n",
        "config = PretrainedConfig.from_pretrained(\"./test-ner\")\n",
        "\n",
        "pipeline = pipeline('ner', model=model, tokenizer=tokenizer, config=config)\n",
        "\n",
        "nlp_grouped = TokenClassificationPipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    grouped_entities=True\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_vRsRjWw3Uo"
      },
      "source": [
        "a = nlp_grouped(\"SAMPLE_START RAMESWARAM : Fishermen call off strike January 05 , 2014 00:00 IST The country boat fishermen in Pam ban , who had\")\n",
        "a\n",
        "#len(a[\"word\"])\n",
        "#a[\"word\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gv3ietNxrMOc"
      },
      "source": [
        "import json\n",
        "with open(\"labels.txt\", \"r\") as f:\n",
        "  label_dict = json.loads(f.read())\n",
        "\n",
        "\n",
        "submission = open(\"submission.txt\", \"w\")\n",
        "with open(\"test.txt\", \"r\") as f:\n",
        "  data = f.readlines()\n",
        "  words = []\n",
        "  \n",
        "  for item in data:\n",
        "    item = item.split()\n",
        "    \n",
        "  \n",
        "    if not item or item[0] == \"[SEP]\":\n",
        "      joined_words = \" \".join(words)\n",
        "      pred = nlp_grouped(joined_words, device=0)\n",
        "      for idx, i in enumerate(pred):\n",
        "        start = i[\"start\"]\n",
        "        end = i[\"end\"]\n",
        "        label_text = joined_words[start:end]\n",
        "        label_text = label_text.split()\n",
        "        for item in pred[idx:]\n",
        "          start = i[\"start\"]\n",
        "          end = i[\"end\"]\n",
        "          label_text = joined_words[start:end]\n",
        "          label_text = label_text.split()\n",
        "\n",
        "        for token in label_text:\n",
        "          submission.write(token + \"\\t\" + label_dict[i[\"entity_group\"]] + \"\\n\")\n",
        "      if not item:\n",
        "        submission.write(\"\\n\")\n",
        "      else:\n",
        "        submission.write(\"[SEP]\" + \"\\t\" + \"O\\n\")\n",
        "\n",
        "       \n",
        "      #print(pred)\n",
        "      #submission.write(json.dumps(d) + \"\\n\")\n",
        "      words = []\n",
        "    else:\n",
        "      words.append(item[0])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPfjLMxpEYgb"
      },
      "source": [
        "nlp_grouped(\"People associated with the agitation and others could also submit their suggestions\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm6hEc5LgC4_"
      },
      "source": [
        "#cat en-train.txt | cut -f 2 | grep -v \"^$\"| sort | uniq > labels.txt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DO9azt5ijKcW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}